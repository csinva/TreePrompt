{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import numpy as np\n",
    "from copy import deepcopy\n",
    "import sys\n",
    "import datasets\n",
    "import imodelsx.metrics\n",
    "import numpy as np\n",
    "sys.path.append('../experiments/')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sys\n",
    "from os.path import join\n",
    "import datasets\n",
    "from typing import Dict, List\n",
    "from dict_hash import sha256\n",
    "import numpy as np\n",
    "\n",
    "import imodelsx.treeprompt.stump\n",
    "\n",
    "from tprompt.compiler.evaluator import PromptHooker, modify_activations\n",
    "from tprompt.compiler import compiling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load prompt vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "checkpoint = 'gpt2'\n",
    "prompts = [\n",
    "    \" This review of a movie is\",\n",
    "    \" Positive or Negative? The movie was\",\n",
    "    \" The sentiment of the movie was\",\n",
    "    # \" The plot of the movie was really\",\n",
    "    \" The acting in the movie was\",\n",
    "\n",
    "]\n",
    "prompt_at_start_or_end = 'start'\n",
    "# note: also requires specifying the layer for the hook below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "avg_soft_prompt = compiling.get_avg_soft_prompt(checkpoint, prompts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tok = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "# tokenize the prompts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "lens = [len(tok.encode(prompt)) for prompt in prompts]\n",
    "\n",
    "# get the index of the longest prompt\n",
    "max_prompt_len = np.argmax(lens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[6, 7, 6, 6]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lens"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65b06cb4e8af46178dea55ce1679fe18",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "dset_train = datasets.load_dataset('rotten_tomatoes')['train']\n",
    "dset_train = dset_train.select(np.random.choice(\n",
    "    len(dset_train), size=1000, replace=False))\n",
    "# dset_val = datasets.load_dataset('rotten_tomatoes')['validation']\n",
    "# dset_val = dset_val.select(np.random.choice(\n",
    "#     len(dset_val), size=100, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 0:  This review of a movie is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              6.89it/s]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------\n",
      "Prompt 0:  This review of a movie is\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              7.41it/s]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 1:  Positive or Negative? The movie was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              7.29it/s]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 2:  The sentiment of the movie was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              7.57it/s]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt 3:  The acting in the movie was\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                              7.45it/s]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "**Averaged** -> 0.549\n",
      "0  This review of a movie is -> 0.483\n",
      "1  Positive or Negative? The movie was -> 0.489\n",
      "2  The sentiment of the movie was -> 0.54\n",
      "3  The acting in the movie was -> 0.519\n"
     ]
    }
   ],
   "source": [
    "verbalizer = {0: \" Negative.\", 1: \" Positive.\"}\n",
    "\n",
    "m = PromptHooker(\n",
    "    checkpoint=checkpoint,\n",
    "    # this should probably be the prompt with the max num tokens?\n",
    "    prompts=[prompts[0]],\n",
    "    verbalizer=verbalizer,\n",
    "    cache_prompt_features_dir=None,\n",
    "    random_state=42,\n",
    "    hook_weights=avg_soft_prompt,\n",
    "    prompt_at_start_or_end='end',\n",
    "    prompt_template=\"{example}{prompt}\",\n",
    ")\n",
    "m.fit(dset_train[\"text\"], dset_train[\"label\"])\n",
    "acc_avg = m.prompt_accs_[0]\n",
    "\n",
    "print('------------------------')\n",
    "m = PromptHooker(\n",
    "    checkpoint=checkpoint,\n",
    "    prompts=prompts,\n",
    "    verbalizer=verbalizer,\n",
    "    cache_prompt_features_dir=None,\n",
    "    random_state=42,\n",
    "    hook_weights=None,\n",
    "    prompt_at_start_or_end='end',\n",
    ")\n",
    "m.fit(dset_train[\"text\"], dset_train[\"label\"])\n",
    "accs0 = deepcopy(m.prompt_accs_)\n",
    "print('**Averaged** ->', acc_avg)\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(i, prompt, '->', accs0[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
