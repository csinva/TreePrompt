{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import viz\n",
    "import transformers\n",
    "import sys\n",
    "import tprompt.utils\n",
    "from os.path import join\n",
    "import datasets\n",
    "from typing import Dict, List\n",
    "from sklearn.tree import plot_tree\n",
    "\n",
    "from abc import ABC, abstractmethod\n",
    "import logging\n",
    "import math\n",
    "import random\n",
    "import imodels\n",
    "import imodelsx.util\n",
    "import imodelsx.metrics\n",
    "import numpy as np\n",
    "import tprompt.utils\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.cuda\n",
    "import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "sys.path.append('../experiments/')\n",
    "\n",
    "from tprompt.api import TreePromptClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = datasets.load_dataset('rotten_tomatoes')['train']\n",
    "dset_train = dset_train.select(np.random.choice(\n",
    "    len(dset_train), size=100, replace=False))\n",
    "dset_val = datasets.load_dataset('rotten_tomatoes')['validation']\n",
    "dset_val = dset_val.select(np.random.choice(\n",
    "    len(dset_val), size=100, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompts = [\n",
    "    \"This movie is\",\n",
    "    \" Positive or Negative? The movie was\",\n",
    "    \" The sentiment of the movie was\",\n",
    "    \" The plot of the movie was really\",\n",
    "    \" The acting in the movie was\",\n",
    "]\n",
    "verbalizer = {0: \" Negative.\", 1: \" Positive.\"}\n",
    "# m = TreePromptClassifier(\n",
    "#     checkpoint=\"gpt2\",\n",
    "#     prompts=prompts,\n",
    "#     verbalizer=verbalizer,\n",
    "#     cache_prompt_features_dir=None, #'cache_prompt_features_dir/gp2',\n",
    "# )\n",
    "# m.fit(dset_train[\"text\"], dset_train[\"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2Model(\n",
      "  (wte): Embedding(50257, 768)\n",
      "  (wpe): Embedding(1024, 768)\n",
      "  (drop): Dropout(p=0.1, inplace=False)\n",
      "  (h): ModuleList(\n",
      "    (0-11): 12 x GPT2Block(\n",
      "      (ln_1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (attn): GPT2Attention(\n",
      "        (c_attn): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (attn_dropout): Dropout(p=0.1, inplace=False)\n",
      "        (resid_dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "      (ln_2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      "      (mlp): GPT2MLP(\n",
      "        (c_fc): Conv1D()\n",
      "        (c_proj): Conv1D()\n",
      "        (act): NewGELUActivation()\n",
      "        (dropout): Dropout(p=0.1, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (ln_f): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model.transformer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "\n",
    "# Initialize the model and tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained('gpt2')\n",
    "model = AutoModelForCausalLM.from_pretrained('gpt2').eval()\n",
    "inputs = tokenizer(\"Hi, my name is\", return_tensors=\"pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dropout(p=0.1, inplace=False)\n",
      " not\n",
      " John\n",
      " a\n",
      " James\n",
      " \"\n",
      " the\n",
      " J\n",
      " David\n",
      " Chris\n",
      ",\n"
     ]
    }
   ],
   "source": [
    "logits_orig = model(**inputs).logits\n",
    "\n",
    "\n",
    "def modify_activations(module, inputs, outputs):\n",
    "    # Define the hook function\n",
    "    print(module)\n",
    "    return outputs * 3  # * 0.01\n",
    "\n",
    "\n",
    "# Add the hook to the first transformer layer\n",
    "hook = model.transformer.drop.register_forward_hook(modify_activations)\n",
    "# hook = model.transformer.h[3].register_forward_hook(change_activations)\n",
    "# Use the model\n",
    "\n",
    "logits_modified = model(**inputs).logits\n",
    "most_probable_tokens = torch.topk(logits_modified, k=10, dim=-1)\n",
    "print('\\n'.join([tokenizer.decode(x)\n",
    "      for x in most_probable_tokens.indices[0, -1]]))\n",
    "\n",
    "\n",
    "# Don't forget to remove the hook when you're done\n",
    "hook.remove()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[ -3.1805,  -3.0734,  -3.0371,  ...,  -1.8651,  -2.2625,  -2.4610],\n",
       "         [-23.9639, -25.1146, -25.5593,  ..., -26.0768, -26.4288, -23.8662],\n",
       "         [-16.8143, -17.9152, -18.2056,  ..., -19.3295, -20.3710, -18.1819],\n",
       "         [ 11.9138,  12.9924,  12.5728,  ...,  12.7233,  10.6170,  12.0346],\n",
       "         [  4.2367,   4.4423,   3.6835,  ...,   4.4055,   4.1055,   5.9551]]],\n",
       "       grad_fn=<SubBackward0>)"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logits_orig - logits_modified"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
