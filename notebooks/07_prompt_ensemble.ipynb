{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import viz\n",
    "import transformers\n",
    "import sys\n",
    "import tprompt.utils\n",
    "from os.path import join\n",
    "import datasets\n",
    "from typing import Dict, List\n",
    "from sklearn.tree import plot_tree\n",
    "import imodelsx.util\n",
    "import imodelsx.metrics\n",
    "import numpy as np\n",
    "import tprompt.utils\n",
    "from scipy.special import softmax\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "import torch.cuda\n",
    "import tqdm\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, AutoModel\n",
    "import torch\n",
    "sys.path.append('../experiments/')\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import transformers\n",
    "import sys\n",
    "from os.path import join\n",
    "import datasets\n",
    "from typing import Dict, List\n",
    "from dict_hash import sha256\n",
    "import joblib\n",
    "import random\n",
    "import numpy as np\n",
    "import sklearn.tree\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "import imodelsx.treeprompt.stump\n",
    "from ensembling import PromptHooker, modify_activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hyperparams\n",
    "checkpoint = 'gpt2'\n",
    "prompts = [\n",
    "    \" This movie is\",\n",
    "    \" Positive or Negative? The movie was\",\n",
    "    \" The sentiment of the movie was\",\n",
    "    \" The plot of the movie was really\",\n",
    "    \" The acting in the movie was\",\n",
    "]\n",
    "# note also requires specifying the layer for the hook below\n",
    "\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint).eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUTPUTS_ALL = {}\n",
    "PROMPT_NUM_GLOBAL = 0\n",
    "\n",
    "\n",
    "def store_activations(module, inputs, outputs):\n",
    "    global OUTPUTS_ALL\n",
    "    global PROMPT_NUM_GLOBAL\n",
    "\n",
    "    # Define the hook function\n",
    "    # print(module)\n",
    "    OUTPUTS_ALL[PROMPT_NUM_GLOBAL] = outputs\n",
    "    # print(OUTPUTS_ALL)\n",
    "    PROMPT_NUM_GLOBAL += 1\n",
    "    return outputs * 3  # * 0.01\n",
    "\n",
    "\n",
    "hook = model.transformer.drop.register_forward_hook(store_activations)\n",
    "for i, prompt in enumerate(prompts):\n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\")\n",
    "    # hook = model.transformer.h[3].register_forward_hook(change_activations)\n",
    "    logits_modified = model(**inputs).logits\n",
    "\n",
    "hook.remove()\n",
    "assert len(OUTPUTS_ALL) == len(prompts)\n",
    "\n",
    "# most_probable_tokens = torch.topk(logits_modified, k=10, dim=-1)\n",
    "# print('\\n'.join([tokenizer.decode(x)\n",
    "#   for x in most_probable_tokens.indices[0, -1]]))\n",
    "# logits_orig = model(**inputs).logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  This movie is torch.Size([1, 3, 768])\n",
      "1  Positive or Negative? The movie was torch.Size([1, 7, 768])\n",
      "2  The sentiment of the movie was torch.Size([1, 6, 768])\n",
      "3  The plot of the movie was really torch.Size([1, 7, 768])\n",
      "4  The acting in the movie was torch.Size([1, 6, 768])\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(prompts)):\n",
    "    print(i, prompts[i], OUTPUTS_ALL[i].shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbalizer = {0: \" Negative.\", 1: \" Positive.\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dset_train = datasets.load_dataset('rotten_tomatoes')['train']\n",
    "dset_train = dset_train.select(np.random.choice(\n",
    "    len(dset_train), size=100, replace=False))\n",
    "dset_val = datasets.load_dataset('rotten_tomatoes')['validation']\n",
    "dset_val = dset_val.select(np.random.choice(\n",
    "    len(dset_val), size=100, replace=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = PromptHooker(\n",
    "    checkpoint='gpt2',\n",
    "    prompts=prompts,\n",
    "    verbalizer=verbalizer,\n",
    "    cache_prompt_features_dir=None,\n",
    "    random_state=42,\n",
    "    hook_weights=[1, 1, 1, 1, 1],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m.fit(dset_train[\"text\"], dset_train[\"label\"])\n",
    "\n",
    "# compute accuracy\n",
    "print('Predicting on validation set...')\n",
    "\n",
    "# compare to accuracy for individual prompts\n",
    "for i, prompt in enumerate(prompts):\n",
    "    print(i, prompt, '->', m.prompt_accs_[i])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
