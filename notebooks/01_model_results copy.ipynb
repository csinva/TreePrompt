{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 722/722 [01:08<00:00, 10.60it/s]\n",
      "/home/chansingh/imodelsx/.venv/lib/python3.11/site-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator DecisionTreeClassifier from version 1.2.1 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n",
      "https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import imodelsx.process_results\n",
    "from collections import defaultdict\n",
    "import numpy as np\n",
    "import viz\n",
    "import sys\n",
    "import notebook_helper\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "sys.path.append('../experiments/')\n",
    "# results_dir = '/home/chansingh/mntv1/tree-prompt/feb25'\n",
    "# results_dir = '/home/chansingh/mntv1/tree-prompt/mar17'\n",
    "results_dir = '/home/chansingh/mntv1/tree-prompt/mar18'\n",
    "\n",
    "r = imodelsx.process_results.get_results_df(results_dir)\n",
    "experiment_filename = '../experiments/01_fit.py'\n",
    "r = imodelsx.process_results.fill_missing_args_with_default(r, experiment_filename)\n",
    "r['mean_llm_calls'] = notebook_helper.add_mean_llm_calls(r)\n",
    "r = r[r.model_name != 'manual_rf']\n",
    "r.to_pickle('../results/tprompt_results.pkl')\n",
    "\n",
    "r = pd.read_pickle('../results/tprompt_results.pkl')\n",
    "ravg = r[r.seed == 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "groupings = ['checkpoint', 'prompt_source', 'verbalizer_num'] #, 'binary_classification']\n",
    "for k, rcurve in ravg.groupby(groupings):\n",
    "    rcurve = rcurve.sort_values(by=['dataset_name', 'model_name', 'mean_llm_calls'])\n",
    "\n",
    "    metric = 'roc_auc_test'\n",
    "    # metric = 'accuracy'\n",
    "    x = 'mean_llm_calls'                # x = 'num_prompts'\n",
    "\n",
    "    viz.plot_perf_curves_individual(rcurve, x=x, xlim=15, metric=metric)\n",
    "    x = rcurve.iloc[0]\n",
    "    plt.suptitle(f'''checkpoint={viz.CHECKPOINTS_RENAME_DICT[x.checkpoint]} prompt_source={x.prompt_source} verb={viz.VERBS_RENAME_DICT[x.verbalizer_num]}''', fontsize='xx-small')\n",
    "    # plt.tight_layout()\n",
    "    # plt.show()\n",
    "\n",
    "    # save a couple\n",
    "    row = rcurve.iloc[0]\n",
    "    if row.checkpoint == 'EleutherAI/gpt-j-6B' and row.prompt_source == 'data_demonstrations' and row.verbalizer_num == 0:\n",
    "        plt.suptitle(f'Model: {viz.CHECKPOINTS_RENAME_DICT[x.checkpoint]}, Prompts: {viz.SOURCE_RENAME_DICT.get(x.prompt_source, x.prompt_source)}', fontsize='x-small')\n",
    "        plt.savefig('../results/figs/perf_curves_gptj.pdf', bbox_inches='tight')\n",
    "    if row.checkpoint == 'gpt2' and row.prompt_source == 'manual' and row.verbalizer_num == 0:\n",
    "        plt.suptitle(f'Model: {viz.CHECKPOINTS_RENAME_DICT[x.checkpoint]}, Prompts: {viz.SOURCE_RENAME_DICT.get(x.prompt_source, x.prompt_source)}', fontsize='x-small')\n",
    "        plt.savefig('../results/figs/perf_curves_gpt2.pdf', bbox_inches='tight')\n",
    "\n",
    "viz.save_figs_to_single_pdf(\"all_results.pdf\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Select individual results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{llllrrr}\n",
      "\\toprule\n",
      " &  &  & model name & Ensemble (Boosting) & Ensemble (Greedy) & TreePrompt \\\\\n",
      "dataset name & checkpoint & prompt source & verbalizer num &  &  &  \\\\\n",
      "\\midrule\n",
      "\\multirow[c]{4}{*}{Emotion} & \\multirow[c]{2}{*}{GPT-2} & Demonstrations & Class names & 0.56 & 0.56 & 0.59 \\\\\n",
      " &  & Human & Class names & 0.76 & 0.63 & 0.77 \\\\\n",
      " & \\multirow[c]{2}{*}{GPT-J} & Demonstrations & Class names & 0.71 & 0.68 & 0.72 \\\\\n",
      " &  & Human & Class names & 0.71 & 0.57 & 0.73 \\\\\n",
      "\\multirow[c]{6}{*}{FPB} & \\multirow[c]{3}{*}{GPT-2} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.62 & 0.60 & 0.74 \\\\\n",
      " &  &  & Yes/no & 0.58 & 0.59 & 0.64 \\\\\n",
      " &  & Human & Class names & 0.77 & 0.72 & 0.81 \\\\\n",
      " & \\multirow[c]{3}{*}{GPT-J} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.72 & 0.72 & 0.76 \\\\\n",
      " &  &  & Yes/no & 0.57 & 0.57 & 0.72 \\\\\n",
      " &  & Human & Class names & 0.80 & 0.77 & 0.86 \\\\\n",
      "\\multirow[c]{6}{*}{IMDB} & \\multirow[c]{3}{*}{GPT-2} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.62 & 0.62 & 0.66 \\\\\n",
      " &  &  & Yes/no & 0.59 & 0.59 & 0.69 \\\\\n",
      " &  & Human & Class names & 0.91 & 0.88 & 0.92 \\\\\n",
      " & \\multirow[c]{3}{*}{GPT-J} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.96 & 0.96 & 0.97 \\\\\n",
      " &  &  & Yes/no & 0.95 & 0.96 & 0.97 \\\\\n",
      " &  & Human & Class names & 0.93 & 0.92 & 0.95 \\\\\n",
      "\\multirow[c]{6}{*}{RT} & \\multirow[c]{3}{*}{GPT-2} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.65 & 0.64 & 0.73 \\\\\n",
      " &  &  & Yes/no & 0.72 & 0.73 & 0.79 \\\\\n",
      " &  & Human & Class names & 0.84 & 0.83 & 0.85 \\\\\n",
      " & \\multirow[c]{3}{*}{GPT-J} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.92 & 0.92 & 0.93 \\\\\n",
      " &  &  & Yes/no & 0.88 & 0.87 & 0.93 \\\\\n",
      " &  & Human & Class names & 0.86 & 0.85 & 0.88 \\\\\n",
      "\\multirow[c]{6}{*}{SST2} & \\multirow[c]{3}{*}{GPT-2} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.77 & 0.78 & 0.80 \\\\\n",
      " &  &  & Yes/no & 0.76 & 0.76 & 0.77 \\\\\n",
      " &  & Human & Class names & 0.88 & 0.87 & 0.88 \\\\\n",
      " & \\multirow[c]{3}{*}{GPT-J} & \\multirow[c]{2}{*}{Demonstrations} & Class names & 0.93 & 0.94 & 0.97 \\\\\n",
      " &  &  & Yes/no & 0.90 & 0.90 & 0.94 \\\\\n",
      " &  & Human & Class names & 0.87 & 0.87 & 0.90 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rd = ravg\n",
    "groupings2 = ['checkpoint', 'prompt_source', 'verbalizer_num', 'dataset_name', 'model_name']\n",
    "\n",
    "dset_names = rd.dataset_name.unique().tolist()\n",
    "mets = defaultdict(list)\n",
    "for k, rcurve in rd.groupby(groupings2):    \n",
    "    # rcurve = rcurve.sort_values(by='accuracy_cv', ascending=False)\n",
    "    # metadata\n",
    "    for kk in groupings2:\n",
    "        mets[kk].append(rcurve.iloc[0][kk])\n",
    "    mets['roc_auc<=5'].append(rcurve[rcurve['mean_llm_calls'] <= 5]['roc_auc_test'].max())\n",
    "\n",
    "df = pd.DataFrame.from_dict(mets)\n",
    "df['dataset_name'] = df['dataset_name'].apply(viz.DSETS_RENAME_DICT_ABBR.get)\n",
    "df['model_name'] = df['model_name'].apply(viz.MODELS_RENAME_DICT.get)\n",
    "df['checkpoint'] = df['checkpoint'].apply(viz.CHECKPOINTS_RENAME_DICT.get)\n",
    "df['prompt_source'] = df['prompt_source'].apply(viz.SOURCE_RENAME_DICT.get)\n",
    "df['verbalizer_num'] = df['verbalizer_num'].apply(viz.VERBS_RENAME_DICT.get)\n",
    "# df['prompt_source'] = df['prompt_source'].apply(viz.PROMPT_SOURCES_RENAME_DICT.get)\n",
    "# print(df.style.hide().to_latex())\n",
    "dfp = df.pivot_table(index=['dataset_name', 'checkpoint', 'prompt_source', 'verbalizer_num'], columns=['model_name'], values='roc_auc<=5').round(2)\n",
    "print(dfp.style.format(precision=2, escape=True).to_latex(hrules=True).replace('NaN', '----').replace('_', ' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean improvement 0.03857142857142858\n"
     ]
    }
   ],
   "source": [
    "print(\n",
    "    \"mean improvement\",\n",
    "    np.mean(\n",
    "        dfp[\"TreePrompt\"]\n",
    "        - dfp.apply(\n",
    "            lambda row: max(row[\"Ensemble (Boosting)\"], row[\"Ensemble (Greedy)\"]),\n",
    "            axis=1,\n",
    "        )\n",
    "    ),\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Apply cross-validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# apply cross validation\n",
    "ravg_cv = (\n",
    "    ravg\n",
    "    .sort_values(by='accuracy_cv', ascending=False)\n",
    "    .groupby(by=['split_strategy', 'dataset_name'])\n",
    "    .first()\n",
    "    .reset_index()\n",
    ")\n",
    "ravg_cv"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".llm",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a9ff692d44ea03fd8a03facee7621117bbbb82def09bacaacf0a2cbc238b7b91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
